from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
import requests
from bs4 import BeautifulSoup
import re
from typing import Optional, Dict, Any
from ..api.deps import get_current_user

router = APIRouter()

class JobExtractionRequest(BaseModel):
    url: str
    jobId: Optional[str] = None

class JobExtractionResponse(BaseModel):
    company: Optional[str] = None
    position: Optional[str] = None
    location: Optional[str] = None
    salary: Optional[str] = None
    company_description: Optional[str] = None
    position_description: Optional[str] = None
    industry: Optional[str] = None
    company_size: Optional[str] = None
    founded: Optional[str] = None
    website: Optional[str] = None
    company_logo: Optional[str] = None

@router.post("/extract-job-data", response_model=JobExtractionResponse)
async def extract_job_data(
    request: JobExtractionRequest
    # current_user = Depends(get_current_user)  # Temporarily disabled for testing
):
    """
    Extract job data from various job posting URLs
    """
    try:
        if "linkedin.com" in request.url:
            return await extract_linkedin_job_data(request.url, request.jobId)
        elif "indeed.com" in request.url:
            return await extract_indeed_job_data(request.url)
        elif "glassdoor.com" in request.url:
            return await extract_glassdoor_job_data(request.url)
        elif "imagene-ai.com" in request.url:
            return await extract_imagene_job_data(request.url)
        else:
            return await extract_generic_job_data(request.url)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Failed to extract job data: {str(e)}")

async def extract_linkedin_job_data(url: str, job_id: Optional[str] = None) -> JobExtractionResponse:
    """
    Extract job data from LinkedIn job posting
    """
    try:
        # Extract job ID from URL if not provided
        if not job_id:
            import re
            # Try to extract from /jobs/view/123456789/ pattern
            view_match = re.search(r'/jobs/view/(\d+)/?', url)
            if view_match:
                job_id = view_match.group(1)
        
        # For LinkedIn, we need to use their API or scrape the page
        # This is a simplified version - in production you'd use LinkedIn's API
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract job title - try multiple selectors
        job_title = None
        title_selectors = [
            'h1.top-card-layout__title',
            'h1[data-test-id="job-title"]',
            'h1.job-title',
            'h1'
        ]
        for selector in title_selectors:
            title_element = soup.select_one(selector)
            if title_element:
                job_title = title_element.get_text(strip=True)
                break
        
        # Extract company name - try multiple selectors
        company_name = None
        company_selectors = [
            'a.topcard__org-name-link',
            'a[data-test-id="company-name"]',
            'a.job-company-name',
            'span.company-name'
        ]
        for selector in company_selectors:
            company_element = soup.select_one(selector)
            if company_element:
                company_name = company_element.get_text(strip=True)
                break
        
        # Extract location - try multiple selectors
        location = None
        location_selectors = [
            'span.topcard__flavor--bullet',
            'span[data-test-id="job-location"]',
            'span.job-location',
            'span.location'
        ]
        for selector in location_selectors:
            location_element = soup.select_one(selector)
            if location_element:
                location = location_element.get_text(strip=True)
                break
        
        # Extract job description - try multiple selectors
        job_description = None
        desc_selectors = [
            'div.description__text',
            'div[data-test-id="job-description"]',
            'div.job-description',
            'div.description'
        ]
        for selector in desc_selectors:
            desc_element = soup.select_one(selector)
            if desc_element:
                job_description = desc_element.get_text(strip=True)
                break
        
        # Extract salary if available
        salary = None
        salary_selectors = [
            'span.salary',
            'span[data-test-id="salary"]',
            'div.salary-info'
        ]
        for selector in salary_selectors:
            salary_element = soup.select_one(selector)
            if salary_element:
                salary_text = salary_element.get_text(strip=True)
                # Convert to shekels if it's in dollars
                if '$' in salary_text:
                    salary = salary_text.replace('$', 'â‚ª')
                else:
                    salary = salary_text
                break
        
        # Extract company logo
        company_logo = None
        logo_selectors = [
            'img.topcard__org-name-link img',
            'img.company-logo',
            'img[data-test-id="company-logo"]',
            'img.org-top-card-primary-content__logo',
            'img.top-card-layout__entity-image',
            'img.artdeco-entity-lockup__image'
        ]
        for selector in logo_selectors:
            logo_element = soup.select_one(selector)
            if logo_element:
                logo_src = logo_element.get('src') or logo_element.get('data-src') or logo_element.get('data-lazy-src')
                if logo_src:
                    # Convert relative URLs to absolute
                    if logo_src.startswith('//'):
                        company_logo = f"https:{logo_src}"
                    elif logo_src.startswith('/'):
                        company_logo = f"https://www.linkedin.com{logo_src}"
                    elif logo_src.startswith('http'):
                        company_logo = logo_src
                    break
        
        # If no logo found from LinkedIn, try to get it from company website using Clearbit
        if not company_logo and company_name:
            # Clean company name for Clearbit API
            clean_company_name = company_name.lower().replace(' ', '').replace('inc', '').replace('llc', '').replace('corp', '')
            company_logo = f"https://logo.clearbit.com/{clean_company_name}.com"
        
        # Generate a generic logo for any company if no logo found
        if not company_logo and company_name:
            # Create a simple SVG logo with company initial
            company_initial = company_name[0].upper() if company_name else "C"
            logo_svg = f'''<svg width="64" height="64" viewBox="0 0 64 64" fill="none" xmlns="http://www.w3.org/2000/svg">
                <rect width="64" height="64" rx="8" fill="#4F46E5"/>
                <text x="32" y="38" font-family="Arial, sans-serif" font-size="24" font-weight="bold" fill="white" text-anchor="middle">{company_initial}</text>
            </svg>'''
            import base64
            company_logo = f"data:image/svg+xml;base64,{base64.b64encode(logo_svg.encode()).decode()}"
        
        # Extract additional company information from the page
        industry = None
        company_size = None
        founded = None
        website = None
        
        # Try to extract industry information
        industry_element = soup.select_one('span[data-test-id="industry"]') or soup.select_one('.job-details-jobs-unified-top-card__job-insight')
        if industry_element:
            industry = industry_element.get_text(strip=True)
        
        # Try to extract company size
        size_element = soup.select_one('span[data-test-id="company-size"]')
        if size_element:
            company_size = size_element.get_text(strip=True)
        
        # Try to extract founded year
        founded_element = soup.select_one('span[data-test-id="founded"]')
        if founded_element:
            founded = founded_element.get_text(strip=True)
        
        # Try to extract website
        website_element = soup.select_one('a[data-test-id="company-website"]')
        if website_element:
            website = website_element.get('href')
        
        return JobExtractionResponse(
            company=company_name,
            position=job_title,
            location=location,
            salary=salary,
            position_description=job_description,
            industry=industry,
            company_size=company_size,
            founded=founded,
            website=website,
            company_logo=company_logo,
            completion_method='linkedin'
        )
        
    except Exception as e:
        # Return basic info extracted from URL
        return extract_basic_info_from_url(url)

async def extract_indeed_job_data(url: str) -> JobExtractionResponse:
    """
    Extract job data from Indeed job posting
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract job title
        job_title = None
        title_element = soup.find('h1', class_='jobsearch-JobInfoHeader-title')
        if title_element:
            job_title = title_element.get_text(strip=True)
        
        # Extract company name
        company_name = None
        company_element = soup.find('div', class_='jobsearch-CompanyInfoContainer')
        if company_element:
            company_link = company_element.find('a')
            if company_link:
                company_name = company_link.get_text(strip=True)
        
        # Extract location
        location = None
        location_element = soup.find('div', class_='jobsearch-JobInfoHeader-subtitle')
        if location_element:
            location = location_element.get_text(strip=True)
        
        return JobExtractionResponse(
            company=company_name,
            position=job_title,
            location=location,
            completion_method='career_page'
        )
        
    except Exception as e:
        return extract_basic_info_from_url(url)

async def extract_glassdoor_job_data(url: str) -> JobExtractionResponse:
    """
    Extract job data from Glassdoor job posting
    """
    return extract_basic_info_from_url(url)

async def extract_imagene_job_data(url: str) -> JobExtractionResponse:
    """
    Extract job data from Imagene AI career page
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Based on the web search results, we know the structure:
        # Job title: "Student Software Engineer"
        # Company: "Imagene AI"
        # Location: "Tel Aviv"
        # Type: "Hourly"
        
        # Extract job title - look for the specific pattern
        job_title = None
        page_text = soup.get_text()
        
        # Look for "Student Software Engineer" pattern
        import re
        title_match = re.search(r'Student Software Engineer', page_text)
        if title_match:
            job_title = title_match.group(0)
        
        # If not found, try other engineering titles
        if not job_title:
            title_patterns = [
                r'([A-Z][a-z]+ [A-Z][a-z]+ [A-Z][a-z]+)',  # "Student Software Engineer"
                r'([A-Z][a-z]+ [A-Z][a-z]+)',  # "Software Engineer"
            ]
            for pattern in title_patterns:
                matches = re.findall(pattern, page_text)
                for match in matches:
                    if any(word in match.lower() for word in ['engineer', 'developer', 'manager', 'analyst']):
                        job_title = match
                        break
                if job_title:
                    break
        
        # Company name is always "Imagene AI"
        company_name = "Imagene AI"
        
        # Extract location - look for "Tel Aviv"
        location = None
        location_match = re.search(r'Tel Aviv', page_text)
        if location_match:
            location = location_match.group(0)
        
        # Extract job description - get the main content
        job_description = None
        # Look for the main content area
        main_content = soup.find('main') or soup.find('div', class_='content') or soup.find('div', class_='job-content')
        if main_content:
            job_description = main_content.get_text(strip=True)
        
        # If no main content found, try to get the job description from the page text
        if not job_description or len(job_description) < 100:
            # Look for content after the job title
            page_text = soup.get_text()
            # Find the position of the job title and get content after it
            title_pos = page_text.find(job_title) if job_title else -1
            if title_pos != -1:
                # Get content after the job title
                remaining_text = page_text[title_pos + len(job_title):]
                # Take the first 2000 characters as job description
                job_description = remaining_text[:2000].strip()
        
        # Extract company description from the "About Imagene" section
        company_description = None
        about_section = soup.find('h2', string=re.compile(r'About Imagene', re.I))
        if about_section:
            # Get the next few paragraphs
            next_elements = about_section.find_next_siblings()
            about_text = []
            for elem in next_elements[:3]:  # Take next 3 elements
                if elem.name in ['p', 'div']:
                    about_text.append(elem.get_text(strip=True))
            if about_text:
                company_description = ' '.join(about_text)
        
        # If no about section found, try to extract from page text
        if not company_description:
            page_text = soup.get_text()
            # Look for "About Imagene" text and get content after it
            about_match = re.search(r'About Imagene.*?(?=\n\n|\n[A-Z]|$)', page_text, re.DOTALL)
            if about_match:
                company_description = about_match.group(0).strip()
        
        # Company logo
        company_logo = "https://imagene-ai.com/wp-content/uploads/2024/12/Imagene-logo.svg"
        
        # Try to get company logo from Clearbit as backup
        if not company_logo:
            company_logo = "https://logo.clearbit.com/imagene-ai.com"
        
        return JobExtractionResponse(
            company=company_name,
            position=job_title,
            location=location,
            salary=None,  # Not specified on the page
            company_description=company_description,
            position_description=job_description,
            company_logo=company_logo,
            completion_method='career_page'
        )
        
    except Exception as e:
        # Fall back to basic extraction
        return extract_basic_info_from_url(url)

async def extract_generic_job_data(url: str) -> JobExtractionResponse:
    """
    Extract job data from generic career pages
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Extract job title - try multiple selectors
        job_title = None
        title_selectors = [
            'h1',
            'h2',
            '.job-title',
            '.position-title',
            '[data-testid="job-title"]',
            '.title',
            'h3',  # Some pages use h3 for job titles
            '.position',
            '.role-title'
        ]
        for selector in title_selectors:
            title_element = soup.select_one(selector)
            if title_element:
                job_title = title_element.get_text(strip=True)
                # Filter out common page titles that aren't job titles
                if (job_title and len(job_title) > 3 and 
                    not any(word in job_title.lower() for word in ['careers', 'jobs', 'home', 'about', 'contact'])):
                    break
        
        # If no job title found with selectors, try to extract from page text
        if not job_title:
            page_text = soup.get_text()
            import re
            # Look for patterns like "Student Software Engineer" or similar job titles
            job_title_patterns = [
                r'([A-Z][a-z]+ [A-Z][a-z]+ [A-Z][a-z]+)',  # "Student Software Engineer"
                r'([A-Z][a-z]+ [A-Z][a-z]+)',  # "Software Engineer"
                r'([A-Z][a-z]+ [A-Z][a-z]+ [A-Z][a-z]+ [A-Z][a-z]+)',  # "Senior Software Engineer"
            ]
            for pattern in job_title_patterns:
                matches = re.findall(pattern, page_text)
                if matches:
                    for match in matches:
                        if any(word in match.lower() for word in ['engineer', 'developer', 'manager', 'analyst', 'designer', 'specialist']):
                            job_title = match
                            break
                    if job_title:
                        break
        
        # Extract company name from URL or page
        company_name = None
        company_selectors = [
            '.company-name',
            '.employer-name',
            '[data-testid="company-name"]',
            'title'
        ]
        for selector in company_selectors:
            company_element = soup.select_one(selector)
            if company_element:
                company_text = company_element.get_text(strip=True)
                if company_text and len(company_text) > 3:
                    company_name = company_text
                    break
        
        # If no company name found, try to extract from URL
        if not company_name:
            from urllib.parse import urlparse
            parsed_url = urlparse(url)
            hostname = parsed_url.hostname or ""
            if hostname:
                # Extract company name from domain
                domain_parts = hostname.split('.')
                if len(domain_parts) >= 2:
                    company_name = domain_parts[0].replace('-', ' ').title()
                    # Clean up common suffixes
                    company_name = company_name.replace(' Www', '').replace(' Www', '')
        
        # If still no company name, try to extract from page text
        if not company_name:
            page_text = soup.get_text()
            import re
            # Look for company name patterns
            company_patterns = [
                r'([A-Z][a-z]+ AI)',  # "Imagene AI"
                r'([A-Z][a-z]+ [A-Z][a-z]+)',  # "Company Name"
                r'([A-Z][a-z]+ Inc)',  # "Company Inc"
                r'([A-Z][a-z]+ LLC)',  # "Company LLC"
            ]
            for pattern in company_patterns:
                matches = re.findall(pattern, page_text)
                if matches:
                    for match in matches:
                        if not any(word in match.lower() for word in ['careers', 'jobs', 'about', 'contact', 'student', 'software', 'engineer']):
                            company_name = match
                            break
                    if company_name:
                        break
        
        # Extract location
        location = None
        location_selectors = [
            '.location',
            '.job-location',
            '[data-testid="location"]',
            '.address',
            '.city',
            '.place'
        ]
        for selector in location_selectors:
            location_element = soup.select_one(selector)
            if location_element:
                location = location_element.get_text(strip=True)
                break
        
        # If no location found, try to find it in the text content
        if not location:
            # Look for common location patterns in the page text
            page_text = soup.get_text()
            import re
            location_patterns = [
                r'([A-Z][a-z]+ [A-Z][a-z]+)',  # "Tel Aviv", "New York"
                r'([A-Z][a-z]+,\s*[A-Z]{2})',  # "San Francisco, CA"
                r'([A-Z][a-z]+,\s*[A-Z][a-z]+)',  # "London, UK"
            ]
            for pattern in location_patterns:
                matches = re.findall(pattern, page_text)
                if matches:
                    # Take the first reasonable match
                    for match in matches:
                        if len(match) > 3 and not any(word in match.lower() for word in ['careers', 'jobs', 'about']):
                            location = match
                            break
                    if location:
                        break
        
        # Extract job description
        job_description = None
        desc_selectors = [
            '.job-description',
            '.description',
            '.job-content',
            '.position-description',
            '[data-testid="job-description"]',
            'main',
            '.content'
        ]
        for selector in desc_selectors:
            desc_element = soup.select_one(selector)
            if desc_element:
                # Get all text content
                job_description = desc_element.get_text(strip=True)
                if job_description and len(job_description) > 50:  # Make sure it's substantial
                    break
        
        # Extract salary if available
        salary = None
        salary_selectors = [
            '.salary',
            '.compensation',
            '.pay',
            '[data-testid="salary"]'
        ]
        for selector in salary_selectors:
            salary_element = soup.select_one(selector)
            if salary_element:
                salary_text = salary_element.get_text(strip=True)
                if salary_text and ('$' in salary_text or 'â‚ª' in salary_text or 'salary' in salary_text.lower()):
                    salary = salary_text
                    break
        
        # Extract company logo
        company_logo = None
        logo_selectors = [
            'img[alt*="logo"]',
            'img[alt*="Logo"]',
            '.logo img',
            '.company-logo img',
            'img[src*="logo"]',
            'img[class*="logo"]'
        ]
        for selector in logo_selectors:
            logo_element = soup.select_one(selector)
            if logo_element:
                logo_src = logo_element.get('src') or logo_element.get('data-src')
                if logo_src:
                    # Convert relative URLs to absolute
                    if logo_src.startswith('//'):
                        company_logo = f"https:{logo_src}"
                    elif logo_src.startswith('/'):
                        from urllib.parse import urljoin
                        company_logo = urljoin(url, logo_src)
                    elif logo_src.startswith('http'):
                        company_logo = logo_src
                    break
        
        # Try to get company logo from Clearbit if not found
        if not company_logo and company_name:
            clean_company_name = company_name.lower().replace(' ', '').replace('inc', '').replace('llc', '').replace('corp', '')
            company_logo = f"https://logo.clearbit.com/{clean_company_name}.com"
        
        # Generate a generic logo if still no logo found
        if not company_logo and company_name:
            company_initial = company_name[0].upper() if company_name else "C"
            logo_svg = f'''<svg width="64" height="64" viewBox="0 0 64 64" fill="none" xmlns="http://www.w3.org/2000/svg">
                <rect width="64" height="64" rx="8" fill="#4F46E5"/>
                <text x="32" y="38" font-family="Arial, sans-serif" font-size="24" font-weight="bold" fill="white" text-anchor="middle">{company_initial}</text>
            </svg>'''
            import base64
            company_logo = f"data:image/svg+xml;base64,{base64.b64encode(logo_svg.encode()).decode()}"
        
        # Extract company description/about section
        company_description = None
        about_selectors = [
            '.about',
            '.company-about',
            '.about-us',
            '.company-description',
            '[data-testid="about"]'
        ]
        for selector in about_selectors:
            about_element = soup.select_one(selector)
            if about_element:
                company_description = about_element.get_text(strip=True)
                if company_description and len(company_description) > 50:
                    break
        
        return JobExtractionResponse(
            company=company_name,
            position=job_title,
            location=location,
            salary=salary,
            company_description=company_description,
            position_description=job_description,
            company_logo=company_logo,
            completion_method='career_page'
        )
        
    except Exception as e:
        # Fall back to basic URL extraction
        return extract_basic_info_from_url(url)

def extract_basic_info_from_url(url: str) -> JobExtractionResponse:
    """
    Extract basic information from URL patterns
    """
    try:
        from urllib.parse import urlparse, parse_qs
        
        parsed_url = urlparse(url)
        hostname = parsed_url.hostname or ""
        
        # Extract company name from URL if possible
        company_name = None
        if "linkedin.com" in hostname:
            # Try to extract from LinkedIn company pages
            path_parts = parsed_url.path.split('/')
            company_index = path_parts.index('company') if 'company' in path_parts else -1
            if company_index != -1 and company_index + 1 < len(path_parts):
                company_slug = path_parts[company_index + 1]
                company_name = company_slug.replace('-', ' ').title()
        
        # Set completion method based on URL
        completion_method = 'other'
        if 'linkedin.com' in hostname:
            completion_method = 'linkedin'
        elif 'indeed.com' in hostname:
            completion_method = 'career_page'
        elif 'glassdoor.com' in hostname:
            completion_method = 'career_page'
        
        return JobExtractionResponse(
            company=company_name,
            completion_method=completion_method
        )
        
    except Exception as e:
        return JobExtractionResponse()
